{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to SharingHub","text":"<p>Welcome to our AI-focused web portal, SharingHub, designed to help you discover, navigate, and analyze your AI-related Git projects hosted on GitLab. Here you can download code, models and datasets with our interface focused on navigation and accessibility!</p> <p>We take inspiration from Hugging Face, the most used AI sharing website in the world, but our goal is turned towards the geospatial component, with GIS capabilities, with datasets and AI models referenced in space and time. This portal is intended to improve exchanges and collaboration between scientists by integrating voting mechanisms, code reviews, and the complete management of the shared assets life cycle.</p>"},{"location":"#features","title":"Features","text":"<p>Warning</p> <p>Because SharingHub itself is configurable, and rely on your GitLab instance, some features may not be available for your instance.</p>"},{"location":"#gitlab-integration","title":"GitLab integration","text":"<p>Very light, SharingHub is deployed on top of a GitLab instance, and dynamically extracts metadata from the shared projects to serve them through its web interface, as well as a normalized API.</p>"},{"location":"#standardized-api","title":"Standardized API","text":"<p>Our API implements the STAC specification, and more precisely the STAC API. This specification is strongly integrated to OGC standards, and we are compliant with the OGC API Features.</p>"},{"location":"#data-storage","title":"Data storage","text":"<p>We can deliver large datasets by offering a DVC remote (S3 store), and support of Git LFS for compatible GitLab instances.</p>"},{"location":"#machine-learning-tracking","title":"Machine learning tracking","text":"<p>With its integration with MLflow, the SharingHub can be used for remote tracking of ML training. The tracking is directly linked to the repository containing the model, to keep the training runs close to the source code.</p>"},{"location":"design/architecture/","title":"Architecture","text":"<p>SharingHub architecture is organized into three main components:</p> <ul> <li>SharingHub UI:</li> </ul> <p>The \"SharingHub UI\" is the main entry point. It takes the form of a web portal specially developed to make it easy to discover, browse, review and download services, code, AI models and datasets. It dynamically extracts the information stored by users and present it in the best possible way once it has been developed/improved.</p> <ul> <li>SharingHub Server:</li> </ul> <p>The \"SharingHub Server\" is the engine that supports all the intelligence that enables the portal to carry out its operations. Its aim is to provide interoperable interfaces that can also be used by external tools, which can then interface with models, datasets and metrics managed. Thus, the portal backend acts like an enhanced SpatioTemporal Asset Catalog (STAC) that dynamically creates its content structure from assets stored in different Git repositories. It serves as a bridge between the Git repositories and the STAC format, enabling users to browse and access datasets within a fully functional STAC structure.</p> <ul> <li>GitLab Instance:</li> </ul> <p>A \"GitLab\" instance, that can include an activated \"MLflow\" service, is specifically configured to meet the needs of users, whether they are developers, AI scientists, or service providers. All the functionalities of GitLab are retained (Git, DevOps, issue management, code review, etc.), making this component usable throughout the development, deployment, and maintenance lifecycle of projects and services to implement best development practices, manage test execution, perform continuous integration, ensure traceability, and reproducibility.</p> SharingHub Architecture"},{"location":"design/philosophy/","title":"Philosophy","text":"<p>SharingHub offers collaborative services for managing code, datasets, and AI models, emphasizing the importance of sharing and collaboration to enhance efficiency, quality, and reuse within EO platform ecosystems. It integrates features like code and model ranking, peer reviews, full lifecycle support, and asset discovery.</p> <p>Key features include:</p> <ul> <li> <p>Lifecycle Management: Tools and functionalities for managing the entire lifecycle of software packages, AI models, workflows, and services, from development and testing to deployment.</p> </li> <li> <p>Source Code and Asset Management: Storage and management of source code, assets, and applications created and published by users.</p> </li> <li> <p>Ranking and Peer Review: Interfaces for ranking algorithms, software packages, datasets, and AI models. The peer review process allows thorough evaluation and feedback, ensuring code quality and preventing unstable code from shipping.</p> </li> </ul> <p>SharingHub aligns with GitLab CE to provide a robust infrastructure, which can include ML/AI lifecycle management capabilities through MLflow integration.</p> <p>To address this, our solution extends GitLab with a dedicated web portal customized for researchers, data scientists, and AI analysts.</p> <p>This extension provides:</p> <ul> <li> <p>User-Friendly Interfaces: Specialized tools and interfaces for presenting services, AI models, datasets, and their interconnections in a scientific manner.</p> </li> <li> <p>Focused Development: Enables AI researchers to concentrate on developing AI models and algorithms while benefiting from GitLab's collaborative, version control features and MLflow service for AI models training.</p> </li> </ul>"},{"location":"explore/api-auth/","title":"Authentication","text":"<p>The authentication with the API is done via GitLab Access Token. You must set the header <code>X-Gitlab-Token</code> for your requests.</p> <p>Examples:</p> <ul> <li>Python <code>requests</code></li> </ul> <pre><code>import requests\n\nrequests.get(\n    \"https://sharinghub.example.com/api/stac/collections/ai-model/items\",\n    headers={\"X-Gitlab-Token\": \"xxxxxxxx\"},\n)\n</code></pre> <ul> <li>STAC client <code>pystac-client</code></li> </ul> <pre><code>from pystac_client import Client\n\ncatalog = Client.open('https://sharinghub.example.com/api/stac', headers={\"X-Gitlab-Token\": \"xxxxxxxx\"})\n</code></pre>"},{"location":"explore/api-reference/","title":"API Reference","text":"<p>The best source of information about the API is the OpenAPI definition of SharingHub. You can find below a Swagger UI built from our <code>openapi.json</code>.</p> <p>"},{"location":"explore/api-stac/","title":"STAC API","text":"<p>All projects on SharingHub are available in STAC format as Collection, FeatureCollection or Feature, thanks to the STAC API. We are also compliant with OGC API - Features - Part 1: Core.</p> <p>In other words, it is possible to interact with the SharingHub using any STAC client such as EODAG, QGIS STAC etc... You will find a list of clients here.</p>"},{"location":"explore/categories/","title":"Categories","text":"<p>The SharingHub is an online platform dedicated to sharing resources and tools in the field of artificial intelligence (AI). It offers users the possibility of sharing and accessing projects from several categories, including:</p> <ul> <li>AI models</li> <li>Datasets</li> <li>Processors</li> <li>Tools and libraries</li> <li>Dashboards</li> </ul> <p>Warning</p> <p>Depending on your SharingHub configuration some categories may not be present for your instance.</p>"},{"location":"explore/categories/#ai-models","title":"AI Models","text":"<p>All projects related to the AI models category are served by the SharingHub in the AI models section, using STAC API <code>collection_id: ai-model</code> metadata.</p> <p></p> <p>An example getting projects from AI models category using the STAC API:</p> <pre><code>curl https://sharinghub.example.com/api/stac/search?collections=ai-model&amp;limit=100\n</code></pre>"},{"location":"explore/categories/#datasets","title":"Datasets","text":"<p>Datasets category groups repositories where large volumes of data are stored for your AI model training. These data repositories are compatible with Git LFS and DVC, which ensures a large storage volume.</p> <p>Datasets are served by the SharingHub in the \"Datasets\" section using STAC API <code>collection_id: dataset</code> metadata.</p> <p></p>"},{"location":"explore/categories/#processors","title":"Processors","text":"<p>Processors are projects generally containing external or internal libraries that you have written to improve the handling of your datasets, or to enhance the learning of your AI models.</p> <p>All processors are served by SharingHub in \"Processors\" section using STAC API <code>collection_id: processor</code> metadata.</p> <p></p>"},{"location":"explore/categories/#toolslibrairies","title":"Tools/Librairies","text":"<p>Tools and libraries are projects used by the developers of the platform, not necessarily AI-related.</p> <p>All tools and librairies are served by SharingHub in \"Tools/Librairies\" section using STAC API <code>collection_id: tools</code> metadata.</p> <p></p>"},{"location":"explore/categories/#dashboards","title":"Dashboards","text":"<p>The Dashboard category is used to share demonstrations for models, with a tool like Streamlit to offer an interactive web interface to play around with a model inputs and check the predicted output.</p> <p>All dashboards are served by SharingHub in \"Dashboards\" section using STAC API <code>collection_id: dashboard</code> metadata.</p> <p></p>"},{"location":"explore/login/","title":"Login","text":"<p>Various login options are available on SharingHub.</p>"},{"location":"explore/login/#gitlab-oauth-2-session","title":"GitLab OAuth 2 session","text":"<p>SharingHub is based on GitLab version (&gt;=)15.11, to which the data is connected. Connection to SharingHub is done via the OAuth2 protocol, which will send you back to GitLab. Once you are connected to GitLab, you will be redirected to SharingHub, connected.</p>"},{"location":"explore/login/#personal-access-token","title":"Personal Access Token","text":"<p>The private access token connection mode lets you connect to sharing via an active access token from GitLab.</p> <p>Note</p> <p>The token may not have the right permissions. Check that the token is valid and has \"api\" scope. To see how it works see more.</p>"},{"location":"explore/login/#default-token","title":"Default token","text":"<p>Your SharingHub may have a default token configured. In this case, you do not need to log in to view some projects, but some features will be limited.</p> <p>Note</p> <p>Depending on the platform configuration, this mode can be deactivated and you will be forced to login.</p>"},{"location":"explore/overview/","title":"Overview","text":"<p>SharingHub is a platform offering a collaborative portal based on GitLab for tracking and exchanging code, datasets and scientific models. Thanks to its interface, it promotes productivity, collaboration and efficient development in the field of AI and machine learning. It offers seamless integration with popular AI frameworks and libraries, streamlined data management capabilities and facilitates the sharing and reuse of AI models and geospatial datasets thanks to its compatibility with the STAC protocol.</p> <p>The SharingHub home page presents the platform's various functionalities and navigation links to access them. Users must log in to access all SharingHub features, although a restricted version is available if the user is not logged in.</p> <p></p>"},{"location":"explore/project-view/","title":"Project view","text":"<p>The main SharingHub view, shown below, is designed to display as much metadata as possible about a project, whether it's an AI model, a dataset, or another category.</p> <p></p>"},{"location":"explore/project-view/#1-titleheader","title":"1. Title/Header","text":"<p>This section is made up of the project's name as described in GitLab and a link that takes you directly to the repository where the project is hosted. You can also find the share and start buttons.</p> <p></p>"},{"location":"explore/project-view/#2-keywordstags","title":"2. Keywords/Tags","text":"<p>This section displays all the keywords and licenses related to the project in GitLab. It's important because it's used as a label to filter out projects with common criteria or specifications. Moreover, tags are used to improve searches, as described in the tag search section.</p>"},{"location":"explore/project-view/#3-helpers","title":"3. Helpers","text":"<p>This group of buttons are helpers.</p> <p></p>"},{"location":"explore/project-view/#stac","title":"STAC","text":"<p>The STAC helper display download code with the STAC API by using Python <code>requests</code> and EODAG, as well as STAC informations.</p> <p></p>"},{"location":"explore/project-view/#dvc","title":"DVC","text":"<p>The DVC button displays configuration and setup of DVC for the project. The DVC service enables you to store large volumes of data. The DVC button is therefore only displayed for projects in the \"Datasets\" category, which have a DVC configuration in the source GitLab project. This button displays the remote DVC configuration link and additional information on the DVC documentation and tutorial.</p> <p></p>"},{"location":"explore/project-view/#mlflow","title":"MLflow","text":"<p>The MLflow helper shows MLflow basic code setup with autolog feature, and display a button to go to the MLflow web interface.</p> <p></p>"},{"location":"explore/project-view/#4-action-buttons","title":"4. Action buttons","text":"<p>The actions buttons are divided in 2 groups. On the left are helpers, on the right features.</p>"},{"location":"explore/project-view/#edit-project","title":"Edit project","text":"<p>Open embedded GitLab code editor to edit the project.</p> <p></p> <p>Clicking will open a new tab:</p> <p></p>"},{"location":"explore/project-view/#open-in-jupyter","title":"Open in Jupyter","text":"<p>If the SharingHub is configured with a notebook service, this button will appear, allowing you to open the project, in particular your AI model in a notebook, for testing or manipulation in a dedicated environment.</p> <p></p> <p>When you click the open button, the Jupyter environment is launched to open the target Notebook. Click on the \"Launch Server\" button. At next page, select \"vreot-cloud\" environment and click on \"Start\" button.</p> <p></p> <p></p> <p></p> <p>Info</p> <p>Configure destination folder</p> <p>The project is cloned by default in your Jupyter environment home folder. If you prefer to customize that behaviour, it is required to set an environment variable into Jupyter. To do so, you have to initialize a user Jupyter configuration file. Into Jupyter, open a terminal and execute:</p> <pre><code>jupyter notebook --generate-config\nvim ~/.jupyter/jupyter_notebook_config.py\n</code></pre> <p>and add into that file:</p> <pre><code>import os\nos.environ[\"NBGITPULLER_PARENTPATH\"] = \"destination_folder\"\n</code></pre> <p>with \"destination_folder\" the relative path from your home folder where you want projects to be cloned.</p> <p>Finally, restart Jupyter for that configuration to be applied.</p>"},{"location":"explore/project-view/#5-tabs","title":"5. Tabs","text":"<p>The tabs allow to switch between multiple views.</p>"},{"location":"explore/project-view/#details-tab","title":"Details Tab","text":"<p>This \"details\" tab displays the project description first. This view highlights all the project's files and resources in the \"assets\" section, and thanks to the \"additional resources\" section it's possible to see references to other projects or resources.</p> <p></p> <p>In some cases, and more generally for datasets, the details tab may show a map with an area corresponding to the geographic extent of the dataset.</p> <p></p>"},{"location":"explore/project-view/#reviews-tab","title":"Reviews Tab","text":"<p>This section is a discussion forum for the project, allowing contributors to leave notes and ask questions to each other.</p> <p></p>"},{"location":"explore/search/","title":"Search","text":"<p>SharingHub is a platform with your AI models, Datasets, Processors, Tools/Libraries, and Dashboards.</p> <p>Everything you need for your Machine Learning projects is accessible through SharingHub which provides several search features.</p> <p></p>"},{"location":"explore/search/#tag-search-tab","title":"Tag search tab","text":"<p>In the category view, it's possible to search for projects by specific tag to target your search. There are two types of tags: common tags, which refers to the most commonly used tags in the field, and other tags, which refers to tags associated with projects in the GitLab connected to SharingHub.</p> Common Other"},{"location":"explore/search/#filter-by-title","title":"Filter by title","text":"<p>The search by title field allows you to dynamically filter projects in the current category page.</p> <p></p>"},{"location":"explore/search/#filter-by-starred-projects","title":"Filter by starred projects","text":"<p>The star button allows to filter only projects you have starred. Projects of any categories can be starred independently on SharingHub or GitLab and this status will be automatically synchronized between both.</p> <p></p> <p>Click on the button again to deactivate this filtering.</p>"},{"location":"explore/search/#sort-items","title":"Sort items","text":"<p>In addition to filtering, it is also possible to sort results to optimize visualization. The following criteria are available for filtering:</p> <ul> <li>Recently created projects,</li> <li>Recently modified projects,</li> <li>Most popular projects (by number of likes).</li> <li>You can also sort them in ascending alphabetical order to facilitate your search.</li> </ul> <p></p>"},{"location":"explore/search/#search-anywhere","title":"Search Anywhere","text":""},{"location":"explore/search/#advanced-search","title":"Advanced Search","text":"<p>Sometimes, you may want to perform complex or more precise searches. The \"advanced search\" view provides a richer interface for your searches.</p> <p></p>"},{"location":"legal/license/","title":"License","text":"<p>SharingHub is an open-source software distributed under the following license:</p> <pre><code>                                 Apache License\n                           Version 2.0, January 2004\n                        http://www.apache.org/licenses/\n\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n   1. Definitions.\n\n      \"License\" shall mean the terms and conditions for use, reproduction,\n      and distribution as defined by Sections 1 through 9 of this document.\n\n      \"Licensor\" shall mean the copyright owner or entity authorized by\n      the copyright owner that is granting the License.\n\n      \"Legal Entity\" shall mean the union of the acting entity and all\n      other entities that control, are controlled by, or are under common\n      control with that entity. For the purposes of this definition,\n      \"control\" means (i) the power, direct or indirect, to cause the\n      direction or management of such entity, whether by contract or\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n      outstanding shares, or (iii) beneficial ownership of such entity.\n\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n      exercising permissions granted by this License.\n\n      \"Source\" form shall mean the preferred form for making modifications,\n      including but not limited to software source code, documentation\n      source, and configuration files.\n\n      \"Object\" form shall mean any form resulting from mechanical\n      transformation or translation of a Source form, including but\n      not limited to compiled object code, generated documentation,\n      and conversions to other media types.\n\n      \"Work\" shall mean the work of authorship, whether in Source or\n      Object form, made available under the License, as indicated by a\n      copyright notice that is included in or attached to the work\n      (an example is provided in the Appendix below).\n\n      \"Derivative Works\" shall mean any work, whether in Source or Object\n      form, that is based on (or derived from) the Work and for which the\n      editorial revisions, annotations, elaborations, or other modifications\n      represent, as a whole, an original work of authorship. For the purposes\n      of this License, Derivative Works shall not include works that remain\n      separable from, or merely link (or bind by name) to the interfaces of,\n      the Work and Derivative Works thereof.\n\n      \"Contribution\" shall mean any work of authorship, including\n      the original version of the Work and any modifications or additions\n      to that Work or Derivative Works thereof, that is intentionally\n      submitted to Licensor for inclusion in the Work by the copyright owner\n      or by an individual or Legal Entity authorized to submit on behalf of\n      the copyright owner. For the purposes of this definition, \"submitted\"\n      means any form of electronic, verbal, or written communication sent\n      to the Licensor or its representatives, including but not limited to\n      communication on electronic mailing lists, source code control systems,\n      and issue tracking systems that are managed by, or on behalf of, the\n      Licensor for the purpose of discussing and improving the Work, but\n      excluding communication that is conspicuously marked or otherwise\n      designated in writing by the copyright owner as \"Not a Contribution.\"\n\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\n      on behalf of whom a Contribution has been received by Licensor and\n      subsequently incorporated within the Work.\n\n   2. Grant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form.\n\n   3. Grant of Patent License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,\n      where such license applies only to those patent claims licensable\n      by such Contributor that are necessarily infringed by their\n      Contribution(s) alone or by combination of their Contribution(s)\n      with the Work to which such Contribution(s) was submitted. If You\n      institute patent litigation against any entity (including a\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.\n\n   4. Redistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n      modifications, and in Source or Object form, provided that You\n      meet the following conditions:\n\n      (a) You must give any other recipients of the Work or\n          Derivative Works a copy of this License; and\n\n      (b) You must cause any modified files to carry prominent notices\n          stating that You changed the files; and\n\n      (c) You must retain, in the Source form of any Derivative Works\n          that You distribute, all copyright, patent, trademark, and\n          attribution notices from the Source form of the Work,\n          excluding those notices that do not pertain to any part of\n          the Derivative Works; and\n\n      (d) If the Work includes a \"NOTICE\" text file as part of its\n          distribution, then any Derivative Works that You distribute must\n          include a readable copy of the attribution notices contained\n          within such NOTICE file, excluding those notices that do not\n          pertain to any part of the Derivative Works, in at least one\n          of the following places: within a NOTICE text file distributed\n          as part of the Derivative Works; within the Source form or\n          documentation, if provided along with the Derivative Works; or,\n          within a display generated by the Derivative Works, if and\n          wherever such third-party notices normally appear. The contents\n          of the NOTICE file are for informational purposes only and\n          do not modify the License. You may add Your own attribution\n          notices within Derivative Works that You distribute, alongside\n          or as an addendum to the NOTICE text from the Work, provided\n          that such additional attribution notices cannot be construed\n          as modifying the License.\n\n      You may add Your own copyright statement to Your modifications and\n      may provide additional or different license terms and conditions\n      for use, reproduction, or distribution of Your modifications, or\n      for any such Derivative Works as a whole, provided Your use,\n      reproduction, and distribution of the Work otherwise complies with\n      the conditions stated in this License.\n\n   5. Submission of Contributions. Unless You explicitly state otherwise,\n      any Contribution intentionally submitted for inclusion in the Work\n      by You to the Licensor shall be under the terms and conditions of\n      this License, without any additional terms or conditions.\n      Notwithstanding the above, nothing herein shall supersede or modify\n      the terms of any separate license agreement you may have executed\n      with Licensor regarding such Contributions.\n\n   6. Trademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.\n\n   7. Disclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, Licensor provides the Work (and each\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n      implied, including, without limitation, any warranties or conditions\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n      PARTICULAR PURPOSE. You are solely responsible for determining the\n      appropriateness of using or redistributing the Work and assume any\n      risks associated with Your exercise of permissions under this License.\n\n   8. Limitation of Liability. In no event and under no legal theory,\n      whether in tort (including negligence), contract, or otherwise,\n      unless required by applicable law (such as deliberate and grossly\n      negligent acts) or agreed to in writing, shall any Contributor be\n      liable to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of such damages.\n\n   9. Accepting Warranty or Additional Liability. While redistributing\n      the Work or Derivative Works thereof, You may choose to offer,\n      and charge a fee for, acceptance of support, warranty, indemnity,\n      or other liability obligations and/or rights consistent with this\n      License. However, in accepting such obligations, You may act only\n      on Your own behalf and on Your sole responsibility, not on behalf\n      of any other Contributor, and only if You agree to indemnify,\n      defend, and hold each Contributor harmless for any liability\n      incurred by, or claims asserted against, such Contributor by reason\n      of your accepting any such warranty or additional liability.\n\n   END OF TERMS AND CONDITIONS\n\n   APPENDIX: How to apply the Apache License to your work.\n\n      To apply the Apache License to your work, attach the following\n      boilerplate notice, with the fields enclosed by brackets \"[]\"\n      replaced with your own identifying information. (Don't include\n      the brackets!)  The text should be enclosed in the appropriate\n      comment syntax for the file format. We also recommend that a\n      file or class name and description of purpose be included on the\n      same \"printed page\" as the copyright notice for easier\n      identification within third-party archives.\n\n   Copyright [yyyy] [name of copyright owner]\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n</code></pre>"},{"location":"legal/privacy/","title":"Privacy policy","text":"<p>We place a paramount importance on the privacy of our users. This privacy policy explains how we collect, use, and protect your informations during your use of our site.</p>"},{"location":"legal/privacy/#information-collection","title":"Information Collection","text":"<p>We do not store any personally identifiable information unless you choose to provide it voluntarily. During your visit to our site, we use session cookies to enhance your browsing experience. These cookies are temporary and managed by your browser, you can delete them anytime.</p>"},{"location":"legal/privacy/#authentication","title":"Authentication","text":"<p>Our authentication system relies on an external integration with GitLab. Therefore, we neither store nor access your login information. Your account details, including your username and password, are managed by GitLab in accordance with their own privacy and security policies.</p>"},{"location":"legal/privacy/#use-of-information","title":"Use of Information","text":"<p>We do not use any personally identifiable information for tracking or profiling activities. Data collected by session cookies is used solely to optimize the site's performance during your browsing session.</p>"},{"location":"legal/privacy/#information-sharing","title":"Information Sharing","text":"<p>We do not share, sell, or rent any personal information with third parties.</p>"},{"location":"legal/privacy/#changes-to-the-privacy-policy","title":"Changes to the Privacy Policy","text":"<p>We reserve the right to update this privacy policy at any time. Any changes will be posted on this page.</p> <p>By using our site, you agree to the terms of our privacy policy. If you have any questions or concerns, feel free to contact us at [contact email address].</p> <p>Last Updated: 04/01/2024</p>"},{"location":"resources/","title":"Resources \ud83d\uddc3\ufe0f","text":"<p>Additional resources, external links and more to better understand the concepts used in SharingHub.</p> <ul> <li>STAC</li> <li>OGC</li> </ul>"},{"location":"resources/ogc/","title":"OGC","text":"<p>With SharingHub, we are committed to the use of open standards, such as OGC.</p> <p>The Open Geospatial Consortium (OGC) is an international consortium of more than 500 member organizations from government, academia, and industry. The primary goal of the OGC is to enable interoperability and seamless integration of geospatial data and services across different platforms, systems, and applications. This is achieved through the development and promotion of consensus-based open standards for geospatial data formats, interfaces, and protocols.</p> <p>Inside of OGC, there is OGC API, which we try to follow as much as possible for our use-cases. The OGC API suite of standards is a modernized approach to geospatial data access and processing, designed to simplify the sharing and use of geospatial information over the web. The OGC API suite consists of several individual standards, each focusing on different aspects of geospatial data access and processing.</p> <p>As of now, with our STAC API we use OGC API - Features, and are planning to incorporate OGC API - Processes for processing.</p> <ul> <li>\ud83d\udd17 OGC</li> <li>\ud83d\udd17 OGC API</li> </ul>"},{"location":"resources/stac/","title":"STAC","text":"<p>The SpatioTemporal Asset Catalog (STAC) is an emerging community-driven specification for organizing geospatial data, particularly satellite imagery and related geospatial assets. It is designed to provide a common language and structure for describing spatiotemporal datasets, making it easier to search, discover, and share geospatial data across different platforms and tools.</p> <p>This specification is the core of SharingHub, where categories are STAC collections, and projects generates STAC Items. By using it, we benefit from its ecosystem, inheriting tools compatible with it.</p> <p>STAC is built on top of existing web technologies and standards, such as JSON and HTTP, making it lightweight, flexible, and easy to implement. It adopts a modular approach, allowing for extensions to support different types of geospatial data and use cases.</p> <p>Key components of STAC include:</p> <ul> <li>Catalogs: STAC catalogs are hierarchical structures that organize collections of   spatiotemporal assets. They provide metadata about the datasets, such as the spatial   and temporal extent, as well as links to individual assets and related resources.</li> <li>Items: STAC items are individual metadata records that describe specific spatiotemporal   assets, such as satellite images or geospatial datasets. They contain information about   the asset's spatial and temporal properties, as well as any additional metadata or links   to data files.</li> <li>Assets: STAC assets represent the actual data files or resources associated with a   spatiotemporal asset. They can include imagery files, data cubes, metadata files,   or any other type of geospatial data resource.</li> <li>Extensions: STAC extensions allow for the customization and extension of the core   specification to support additional metadata fields, properties, or use cases.   Examples of extensions include support for point cloud data, radar imagery,   or additional metadata fields.</li> </ul> <p>Learn more:</p> <ul> <li>\ud83d\udd17 STAC website</li> <li>\ud83d\udd17 STAC Spec</li> <li>\ud83d\udd17 STAC API Spec</li> <li>\ud83d\udd17 STAC Extensions</li> <li>\ud83d\udd17 STAC API Extensions</li> <li>\ud83d\udd17 STAC Lint</li> </ul>"},{"location":"share/overview/","title":"Overview","text":"<p>You may want to not only Explore the SharingHub, but also share your own work, because the SharingHub cannot be what it is without people sharing their work.</p> <p>The following chapters are here for that purpose! You will learn how to setup your GitLab repository in order to share it with efficiency, by answering these questions:</p> <ul> <li>How to describe my project  with Metadata.</li> <li>How do do I store my (sometimes heavy) Data.</li> </ul> <p>Join us in this collaborative journey \ud83d\ude80 !</p>"},{"location":"share/data/","title":"Data","text":"<p>Whether for AI models, with weight files, or even datasets, there is a real need for storing big data. In SharingHub, we rely on GitLab, and therefore by definition, Git. But Git is not intended to store large volumes of data. We therefore offer two solutions to this problem:</p> <ul> <li> Git LFS</li> <li> DVC</li> </ul>"},{"location":"share/data/dvc/","title":"DVC","text":"<p>DVC (Data Version Control) is an open-source version control system specifically designed for machine learning (ML) and data science projects. It complements traditional version control systems like Git by focusing on managing the large datasets and machine learning models typically used in these domains. DVC provides a simple yet powerful set of tools to track changes, collaborate, and reproduce experiments with data and ML models efficiently. DVC allows you to version control datasets alongside your code. It stores lightweight metafiles in Git to track changes to datasets, while the actual data files are stored separately, typically in cloud storage or network-attached storage (NAS).</p> <p>Your deployment of SharingHub may support DVC, and offer a DVC remote with an S3 storage behind. If a project uses DVC, you can see the DVC button being displayed.</p> <ul> <li> Tutorial \"Dataset with DVC\"</li> <li>\ud83d\udd17 Official website</li> <li>\ud83d\udd17 Documentation</li> </ul>"},{"location":"share/data/git-lfs/","title":"Git LFS","text":"<p>Git LFS (Large File Storage) is an extension for Git, a widely used version control system, designed to handle large files more efficiently. As the name suggests, Git LFS is primarily used for storing large binary files, such as images, audio/video files, datasets, and other non-textual data, which traditional Git struggles to manage effectively due to its inherent limitations.</p> <p>If your GitLab instance supports Git LFS, SharingHub can work with it, and files stored with LFS in the STAC Item's assets are downloadable just like any other file.</p> <ul> <li>\ud83d\udd17 Official website</li> </ul> <p>Note</p> <p>While Git LFS can handle larger files than Git, the size limit per repository is by default of 5GB. For really big data volumes, another supported solution can be used, DVC.</p>"},{"location":"share/examples/dataset-case/","title":"Case: Dataset","text":"<p>The INRIA Aerial Image Labeling dataset is comprised of 360 RGB tiles of 5000\u00d75000px with a spatial resolution of 30cm/px on 10 cities across the globe.</p> <p></p>"},{"location":"share/examples/dataset-case/#configuration","title":"Configuration","text":"<p>To share your dataset on the SharingHub, you need to set up your GitLab repository to include the topics <code>sharinghub:dataset</code> from Settings and General:</p> <p></p> <p>To make your dataset usable by others, you need to create a <code>README.md</code> file. This file should begin with a YAML section describing your dataset's metadata, followed by a markdown section:</p> <ul> <li>The markdown part of your README must contain all useful information about the dataset: how to use it and in what context, how it was created etc...</li> <li>The YAML section is delimited by three <code>---</code> at the top of your file and at the end of the section. It contains the metadata presented in the [Reference].</li> </ul>"},{"location":"share/examples/dataset-case/#structure","title":"Structure","text":"<p>The repository tree:</p> <pre><code>.\n\u251c\u2500\u2500 data\n\u2502   \u251c\u2500\u2500 test\n\u2502   \u2502   \u251c\u2500\u2500 gt.dvc\n\u2502   \u2502   \u2514\u2500\u2500 img.dvc\n\u2502   \u251c\u2500\u2500 train.dvc\n\u2502   \u2514\u2500\u2500 validation.dvc\n\u251c\u2500\u2500 inria_dataset.py\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 requirements.txt\n\u2514\u2500\u2500 samples\n    \u251c\u2500\u2500 ground_truth.png\n    \u2514\u2500\u2500 image.png\n</code></pre> <p>You may notice the \"dvc\" extensions, this is because we use DVC to store the files. Learn more in the tutorial \"Dataset with DVC\".</p>"},{"location":"share/examples/dataset-case/#metadata","title":"Metadata","text":"<p>Here's the project metadata:</p> README.md Metadata<pre><code>assets:\n- \"*.zip\"\n- \"*.py\"\n\ngsd: 0.3\n\nlabel:\n  type: vector\n  properties: null\n  description: \"Ground truth data for two semantic classes: 'building' and 'not building'\"\n  classes:\n    - name: Other\n      classes: [0]\n    - name: Building\n      classes: [255]\n  tasks:\n    - Semantic Segmentation\n</code></pre> <p>Let\u2019s break down the project's metadata.</p> <ul> <li><code>assets</code>: define the files in the repository that we want to share with SharingHub. [Ref]</li> <li><code>gsd</code>: pure STAC property. [Ref]</li> <li><code>label</code>: a STAC extension, adapted to the dataset use-case. [Ref]</li> </ul>"},{"location":"share/examples/model-case/","title":"Case: AI Model","text":"<p>We will study the case of a model used for building detection on Aerial image.</p> <p></p>"},{"location":"share/examples/model-case/#configuration","title":"Configuration","text":"<p>To share your artificial intelligence model on the SharingHub, you need to set up your GitLab repository to include the topics <code>sharinghub:aimodel</code> from Settings and General:</p> <p></p> <p>To make your model usable by others, you need to create a <code>README.md</code> file. This file should begin with a YAML section describing your model's metadata, followed by a markdown section:</p> <ul> <li>The markdown part of your README must contain all the elements needed to train and/or make an inference with your AI model!</li> <li>The YAML section is delimited by three <code>---</code> at the top of your file and at the end of the section. It contains the metadata presented in the [Reference].</li> </ul>"},{"location":"share/examples/model-case/#structure","title":"Structure","text":"<p>The repository tree:</p> <pre><code>.\n\u251c\u2500\u2500 building_process.py\n\u251c\u2500\u2500 inference.ipynb\n\u251c\u2500\u2500 pretrained_weight.pt\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 requirements.txt\n\u2514\u2500\u2500 train.ipynb\n</code></pre>"},{"location":"share/examples/model-case/#metadata","title":"Metadata","text":"<p>Here's the project metadata:</p> README.md Metadata<pre><code>title: Unet neural network for building detection on Aerial image\n\nrelated:\n  dataset: https://gitlab.example.com/&lt;dataset-project-repository&gt;\n\nassets:\n- \"*.ipynb\"\n- \"*.pt\"\n\ngsd: 0.3\nplatform: aerial\n\nproviders:\n  - name: CS Group\n    roles: [\"producer\"]\n    url: https://www.csgroup.eu\n\nlabel:\n    properties: [\"Others\", \"Building\"]\n    description: \"Building detection on Aerial image\"\n    type: \"vector\"\n    classes: [\n        {\n            \"name\": \"Others\",\n            \"classes\":  [0]\n        },\n        {\n            \"name\": \"Building\",\n            \"classes\": [1]\n        }\n    ]\n\nml-model:\n    type: \"ml-model\"\n    learning_approach: \"supervised\"\n    prediction_type: \"segmentation\"\n    architecture: \"U-Net\"\n</code></pre> <p>Let\u2019s break down the project's metadata.</p> <ul> <li><code>title</code>: override the default title, which is the name of the GitLab project \"Unet Building Footprint Segmentation Aerial Image\". [Ref]</li> <li><code>assets</code>: define the files in the repository that we want to share with SharingHub. [Ref]</li> <li><code>gsd</code> and <code>platform</code> are pure STAC properties. [Ref]</li> <li><code>providers</code>: override the default providers. [Ref]</li> <li><code>label</code> and <code>ml-model</code> are STAC extensions. Our model here uses the adapted STAC extensions with its use-case, label and ml-model. [Ref]</li> </ul>"},{"location":"share/metadata/","title":"Metadata","text":"<p>Metadata refers to data that provides information about other data. It offers context, description, and structure to aid in the understanding, organization, and management of data.</p> <p>To understand our metadata, knowledge of basic STAC Concepts is recommended. If you already know how STAC works, you can checkout how to configure a project.</p>"},{"location":"share/metadata/configuration/","title":"Configuration","text":"<p>We rely on user-defined metadata to pass information to SharingHub. The metadata are always handled the same way, regardless of the project's category, although some of them are more suited for specific categories. We will not detail every feature available, for a more in-depth insight of the metadata configuration refer to our Reference.</p>"},{"location":"share/metadata/configuration/#syntax","title":"Syntax","text":"<p>The metadata of a project are defined in its <code>README.md</code> file. Markdown allows a syntax for metadata, as YAML:</p> README.md<pre><code>---\nfoo: bar\n---\n\n# Title\n\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Morbi neque neque,\nmalesuada vel sodales eu, pharetra sit amet nibh. Integer in nunc ante.\nMorbi commodo metus est, id aliquet odio cursus id. Ut non sagittis metus.\n</code></pre> <p>By using a <code>---</code> delimited section at the start of your <code>README.md</code>, you can write metadata that will not be rendered in the project description on SharingHub. The informations here will be parsed and used to process the STAC Item of the project.</p> <p>Warning</p> <p>We always require a <code>README.md</code> file, with the uppercase file name, and in CommonMark (Markdown) format.</p>"},{"location":"share/metadata/configuration/#usage","title":"Usage","text":""},{"location":"share/metadata/configuration/#overview","title":"Overview","text":"Name Default value Configuration Title Name of the GitLab project. Metadata <code>title</code>. Description The project description is the README content for the project view, but for the project card in the list we prioritize the GitLab project description (General Settings). - Preview - Metadata <code>preview</code> or in the \"README.md\" as a link named \"Preview\". License - Metadata <code>license</code> or the license detected by GitLab. Temporal Extent For the temporal extent the start is the datetime of the project creation, and the end is the project last activity datetime Metadata <code>extent.temporal</code>. Spatial Extent - Metadata <code>extent.spatial</code>. Providers The \"host\" provider is linked to the GitLab project URL, and the \"producer\" provider is the GitLab top-level namespace of the project. Metadata <code>providers</code>. Links The default links are STAC recommended links, <code>self</code>, <code>parent</code>, <code>root</code>, <code>collection</code>, and a link to the project bug tracker as well as the license if one is found. Metadata <code>links</code> and <code>related</code>. Note: values will be added and not replace defaults. Assets Inherit global configuration for the project's category. Metadata <code>assets</code>. Extensions Inherit global configuration. Metadata <code>extensions</code>."},{"location":"share/metadata/configuration/#example","title":"Example","text":"<p>A small example to better understand the structure:</p> Metadata example<pre><code>title: My Project\npreview: https://avatars.githubusercontent.com/u/6223127\nlicense: MIT\n\nextent:\n  temporal: [\"2023-11-01T12:00:00.0\", \"2024-01-01T12:00:00.0\"]\n  spatial: [-66.5902, 17.9823, -66.6407, 18.0299]\n\nproviders:\n  - name: CS Group\n    roles: [\"producer\"]\n    url: https://www.csgroup.eu\n  - name: GitLab CS\n    roles: [\"host\"]\n    url: https://gitlab.example.com\n\nrelated:\n  ai-model: https://gitlab.example.com/&lt;ai-model-project-repository&gt;\n\nassets:\n  - \"*.py\"\n</code></pre>"},{"location":"share/metadata/reference/","title":"Reference","text":"<p>This page references all of our Metadata-related available features.</p>"},{"location":"share/metadata/reference/#syntax","title":"Syntax","text":"<p>The metadata of a project are defined in its <code>README.md</code> file. Markdown allows a syntax for metadata, as YAML:</p> README.md<pre><code>---\nfoo: bar\n---\n\n# Title\n\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Morbi neque neque,\nmalesuada vel sodales eu, pharetra sit amet nibh. Integer in nunc ante.\nMorbi commodo metus est, id aliquet odio cursus id. Ut non sagittis metus.\n</code></pre> <p>By using a <code>---</code> delimited section at the start of your <code>README.md</code>, you can write metadata that will not be rendered in the project description on SharingHub. The informations here will be parsed and used to process the STAC Item of the project.</p> <p>Warning</p> <p>We always require a <code>README.md</code> file, with the uppercase file name, and in CommonMark (Markdown) format.</p>"},{"location":"share/metadata/reference/#rendering","title":"Rendering","text":"<p>Our metadata schema relies a lot on STAC, but some helpers are available. All metadata are written inside the <code>---</code> section.</p>"},{"location":"share/metadata/reference/#title","title":"Title","text":"<p>The title defaults to the project name, but you can easily override it with the <code>title</code> metadata.</p> Metadata example<pre><code>title: My Custom Title\n</code></pre>"},{"location":"share/metadata/reference/#preview","title":"Preview","text":"<p>The preview can be auto-discovered in the README, if your image \"alt\" is \"Preview\" (not case-sensitive).</p> README.md<pre><code>![Preview](&lt;url&gt;)\n</code></pre> <p>It can also be auto-discovered if the following files are found in the repository: <code>preview.png</code>, <code>preview.jpg</code>, <code>preview.jpeg</code>.</p> <p>Or, you can define it in the metadata.</p> Metadata<pre><code>preview: &lt;url&gt;\n</code></pre>"},{"location":"share/metadata/reference/#license","title":"License","text":"<p>We can retrieve the license if you have a LICENSE file in your repository, it will be automatically detected. However, it can still be configured.</p> Metadata<pre><code>license: &lt;license name&gt;\n</code></pre> <p>Warning</p> <p>Only a SPDX License Identifier is allowed.</p> <p>In addition, you can set an URL for the license. While it is automatically set to the URL of the project LICENSE file, or determined from the license given, you can also change it:</p> Metadata<pre><code>license-url: &lt;license url&gt;\n</code></pre> <p>Warning</p> <p>The STAC Item specification indicates that the URL of the license SHOULD be defined.</p>"},{"location":"share/metadata/reference/#extent","title":"Extent","text":""},{"location":"share/metadata/reference/#temporal","title":"Temporal","text":"<p>By default, a project temporal extent starts at the creation date of the gitlab repository, and ends at the date of last activity in this repository. But we do have search capabilities on datetime range, so you may want sometime to correct your project temporal extent. Do so with:</p> Metadata example<pre><code>extent:\n  temporal: [\"2023-11-01T12:00:00.0\", \"2024-01-01T12:00:00.0\"]\n</code></pre> <p>Info</p> <p>Datetime should be expressed in UTC.</p>"},{"location":"share/metadata/reference/#spatial","title":"Spatial","text":"<p>You may define a spatial extent for your project.</p> <p>We support bbox:</p> Metadata example<pre><code>extent:\n  spatial: [-66.5902, 17.9823, -66.6407, 18.0299]\n</code></pre> <p>And WKT geometries:</p> Metadata example<pre><code>extent:\n  spatial: \"POLYGON ((-66.6407 17.9823, -66.6407 18.0299, -66.5902 18.0299, -66.5902 17.9823, -66.6407 17.9823))\"\n</code></pre>"},{"location":"share/metadata/reference/#providers","title":"Providers","text":"<p>The provider concept in STAC help to inform about the organizations capturing, producing, processing, hosting or publishing the data of the project.</p> <p>By default, the \"host\" provider is set to the project GitLab URL, and the \"producer\" provider is set to the GitLab repository user or first-level group.</p> <p>Example: for <code>https://gitlab.example.com/A/repo-b</code> with the user \"A\" having a repository \"repo-b\", the host is <code>https://gitlab.example.com/A/repo-b</code>, and the producer is <code>https://gitlab.example.com/A</code>.</p> <p>However, you can still pass your own values for the providers. You must follow the spec of the provider object.</p> Metadata<pre><code>providers:\n  - name: &lt;producer name&gt;\n    roles: [\"producer\"]\n    url: &lt;producer url&gt;\n  - name: &lt;host name&gt;\n    roles: [\"host\"]\n    url: &lt;host url&gt;\n</code></pre>"},{"location":"share/metadata/reference/#links","title":"Links","text":""},{"location":"share/metadata/reference/#related-projects","title":"Related projects","text":"<p>You may want to add links to your projects. A common use-case would be linking an AI model to a dataset. You can link projects between them with the <code>related</code> metadata, but beware, in SharingHub, projects are categorized, and you will need to use that information. SharingHub categories are STAC collections, you will have to use the collection id (example: ai-model, dataset).</p> Metadata<pre><code>related:\n  &lt;collection id&gt;: &lt;gitlab project url&gt;\n</code></pre> <p>List are also accepted for multiple links</p> Metadata<pre><code>related:\n  &lt;collection id&gt;:\n    - &lt;gitlab project 1 url&gt;\n    - &lt;gitlab project 2 url&gt;\n</code></pre>"},{"location":"share/metadata/reference/#raw-links","title":"Raw links","text":"<p>You can also use a more \"low-level\" metadata to directly define additional STAC links with <code>links</code> metadata. Follow the spec.</p> Metadata<pre><code>links:\n  - href: &lt;url&gt;\n    rel: &lt;rel&gt;\n</code></pre> <p>Tip</p> <p>Check Helpers to learn some extra features for the link's <code>href</code>.</p>"},{"location":"share/metadata/reference/#assets","title":"Assets","text":""},{"location":"share/metadata/reference/#basic-usage","title":"Basic usage","text":"<p>You will want to share files from your GitLab repository with SharingHub. Only the files defined in the <code>assets</code> metadata will be shared, to avoid bloating.</p> <p>You can share a file:</p> Metadata example<pre><code>assets:\n  - \"requirements.txt\"\n</code></pre> <p>But sometimes, you will have the need to include multiple files, in this case you can use basic glob syntax with wildcards.</p> Metadata example<pre><code>assets:\n  - \"*.py\"\n</code></pre> <p>All matching files will be added.</p> <p>Note</p> <p>Your SharingHub instance can have some assets rules pre-configured, this is why you may find your project having assets without any configured.</p>"},{"location":"share/metadata/reference/#advanced-usage","title":"Advanced usage","text":"<p>There is an advanced, more powerful syntax available to define your assets. This feature will be very useful for some use-cases.</p>"},{"location":"share/metadata/reference/#media-type","title":"Media type","text":"<p>The STAC asset <code>type</code> is guessed from the file extension, but sometimes you will want to change this type. In the following example we will declare the media type of our COG <code>.tiff</code> files, because it is only detected as <code>image/tiff</code>.</p> Metadata example<pre><code>assets:\n  - glob: \"*.tiff\"\n    type: image/tiff; application=geotiff; profile=cloud-optimized\n</code></pre> <p>There is a shortcut feature available for some file extensions, <code>type-as</code>.</p> Metadata example<pre><code>assets:\n  - glob: \"*.tiff\"\n    type-as: cog\n</code></pre> <p>Available <code>type-as</code> values:</p> type-as type <code>compose</code> <code>text/x-yaml; application=compose</code> <code>cog</code> <code>image/tiff; application=geotiff; profile=cloud-optimized</code> <code>geojson</code> <code>application/geo+json</code> <code>geotiff</code> <code>image/tiff; application=geotiff</code> <code>json</code> <code>application/json</code> <code>notebook</code> <code>application/x-ipynb+json</code> <code>text</code> <code>text/plain</code> <code>xml</code> <code>application/xml</code> <code>yaml</code> <code>text/x-yaml</code> <code>zip</code> <code>application/zip</code>"},{"location":"share/metadata/reference/#custom-assets","title":"Custom assets","text":"<p>There may be times where you will want to add a custom asset. When a <code>glob</code> or <code>path</code> field is provided, we will try to match it with one of the project's files. In this case, we will loop over each match and create an asset with its <code>href</code> set to the file. But you can define assets that do not \"match\" the files. The thing is, we consider an asset valid if it has at least a <code>key</code> and a <code>href</code>. This let you add custom assets to reference external resources.</p> <p>As an example, we will add an asset from this simple STAC Item example of the stac spec repository.</p> Metadata example<pre><code>assets:\n  - key: visual\n    href: https://storage.googleapis.com/open-cogs/stac-examples/20201211_223832_CS2.tif\n    type: image/tiff; application=geotiff; profile=cloud-optimized\n    title: 3-Band Visual\n    roles: [visual]\n</code></pre> <p>Notice that because our <code>assets</code> metadata is a list and not a mapping, we add the keyword <code>key</code> to define the asset key in the resulting asset mapping. Other than that, all STAC asset fields can always be set manually, <code>href</code>, <code>type</code>, <code>title</code>, <code>description</code>,  and <code>roles</code>.</p> <p>Tip</p> <p>Check Helpers to learn some extra features for the asset's <code>href</code>.</p>"},{"location":"share/metadata/reference/#alter-files-assets","title":"Alter files assets","text":"<p>By default, an asset generated from a file only have an <code>href</code>, a <code>roles: [data]</code>, and a <code>type</code> if it was guessed. But, don't hesitate to improve your assets by adding a <code>title</code> an a <code>description</code>! We offer some utility features to alter your assets per match.</p> Metadata example<pre><code>assets:\n  - glob: \"*.py\"\n    key: \"python://{path}\"\n    title: \"File '{path}'\"\n    description: \"Python file '{key}'\"\n</code></pre> <p>A file asset key in the assets mapping is by default the path of the file, but you can change it. You can use <code>{path}</code> in <code>key</code>, <code>title</code>, <code>description</code> and <code>{key}</code> in <code>title</code>, <code>description</code>. This will let you interpolate the value for each match, and is really helpful here to change the asset key for each of the python files.</p>"},{"location":"share/metadata/reference/#extensions","title":"Extensions","text":"<p>We rely on STAC Extensions to enrich our metadata. You can use about every STAC extension you want like this:</p> Metadata<pre><code>extensions:\n  &lt;extension prefix&gt;: &lt;extension schema&gt;\n\n&lt;extension prefix&gt;:\n  &lt;prop&gt;: &lt;val&gt;\n</code></pre> <p>Some STAC Extensions are pre-configured, with no need to specify them in <code>extensions</code> metadata. List of extensions pre-configured:</p> Name Source Prefix Electro-Optical https://github.com/stac-extensions/eo <code>eo</code> Label https://github.com/stac-extensions/label <code>label</code> ML Model https://github.com/stac-extensions/ml-model <code>ml-model</code> Scientific Citation https://github.com/stac-extensions/scientific <code>sci</code> <p>A concrete example for the Scientific Citation Extension:</p> Metadata example<pre><code>sci:\n  doi: 10.XXXX/XXXXX\n  citation: Lorem ipsum dolor sit amet, consectetur adipiscing elit.\n</code></pre> <p>You may have noticed that in STAC Extensions the properties are used like \"sci:doi\", \"sci:citation\", and you are right, but the syntax above help to avoid having to type multiple times the prefix of the extension.</p>"},{"location":"share/metadata/reference/#scientific-citation","title":"Scientific Citation","text":"<p>This extension is built-in in STAC Browser with an interesting parsing capability. You can define their values like in the example above, or write them in your README directly.</p> README.md<pre><code>[DOI: Lorem ipsum dolor sit amet, consectetur adipiscing elit.](https://doi.org/10.XXXX/XXXXX)\n</code></pre> <p>If a markdown link href starts with <code>https://doi.org</code>, it will be retrieved as the DOI. If more than one link matches this condition, the first is still the project DOI, and the remaining DOIs will be added as publications, following the extension.</p>"},{"location":"share/metadata/reference/#remaining-properties","title":"Remaining properties","text":"<p>After every metadata described in the previous sections are processed, there may still remain other metadata. These metadata are passed as-is to the STAC Item properties.</p> <p>For example, you may want declare the <code>gsd</code> or <code>platform</code> of a Sentinel 2 dataset:</p> Metadata example<pre><code>gsd: 10\nplatform: sentinel-2\n</code></pre> <p>These metadata will be passed as properties transparently.</p> Generated STAC Item<pre><code>{\n  ...\n  \"properties\": {\n    ...\n    \"gsd\": 10,\n    \"platform\": \"sentinel-2\",\n    ...\n  }\n  ...\n}\n</code></pre>"},{"location":"share/metadata/reference/#helpers","title":"Helpers","text":"<p>Both Links and Assets declare a <code>href</code> field, an URL to a resource. But, there are two cases where you don't have direct URL to the resource.</p>"},{"location":"share/metadata/reference/#repository-file","title":"Repository file","text":"<p>If an href is detected to be a \"local path\", like \"./myfile.txt\" or \"files/myfile.txt\", the <code>href</code> is automatically changed to an URL using our download API for the file.</p>"},{"location":"share/metadata/reference/#other-project","title":"Other project","text":"<p>The <code>href</code> should never be a hard link to a SharingHub instance, and you may not want to use the <code>related</code> metadata in some cases. You can still generate an URL to the STAC Item of another project without hard linking the SharingHub instance.</p> <p>Because a project's STAC Item is always in a collection (our categories), you will need the \"collection id\" / category of the target project, and you can use the following syntax:</p> Metadata<pre><code>href: &lt;collection id&gt;+&lt;gitlab project url&gt;\n</code></pre> <p>A more concrete example:</p> Metadata example<pre><code>href: dataset+https://gitlab.example.com/space_applications/mlops-services/sharinghub-tests/dataset-sample\n</code></pre> <p>In SharingHub, an URL to the STAC Item will then be created.</p>"},{"location":"share/metadata/stac-concepts/","title":"STAC concepts","text":"<p>Because SharingHub relies on the STAC specification, with a project being transformed to a STAC Item, metadata are tightly tied to it. You can find some resources related to STAC in Resources / STAC.</p> <p>Info</p> <p>A STAC Item is a valid GeoJSON Feature. \u27a1\ufe0f Item spec.</p>"},{"location":"share/metadata/stac-concepts/#properties","title":"Properties","text":"<p>In STAC, the properties of a STAC Item are the closest thing to the concept of metadata. Its mostly a mapping of key-value pair.</p> STAC Item<pre><code>{\n    ...\n    \"properties\": {\n        \"&lt;key&gt;\": \"&lt;value&gt;\"\n    }\n    ...\n}\n</code></pre>"},{"location":"share/metadata/stac-concepts/#links","title":"Links","text":"<p>The \"links\" in a STAC object is a list of URLs, with for each a \"relationship\" (<code>rel</code>) to the item, and a <code>type</code>, which is the media type of the resources pointed by the URL (<code>href</code>).</p> STAC Item<pre><code>{\n    ...\n    \"links\": [\n        ...\n        {\n            \"rel\": \"&lt;value&gt;\",\n            \"href\": \"&lt;url&gt;\",\n            \"type\": \"&lt;media type&gt;\",\n        },\n        ...\n    ]\n    ...\n}\n</code></pre>"},{"location":"share/metadata/stac-concepts/#assets","title":"Assets","text":"<p>The assets of a STAC Item are essentially links to the resources. Unlike links, it is a mapping, with unique keys.</p> STAC Item<pre><code>{\n    ...\n    \"assets\": {\n        ...\n        \"&lt;key&gt;\": {\n            \"href\": \"&lt;url&gt;\",\n            \"title\": \"&lt;string&gt;\",\n            \"roles\": [\"&lt;string&gt;\", ...],\n            \"type\": \"&lt;media type&gt;\",\n        },\n        ...\n    }\n    ...\n}\n</code></pre>"},{"location":"share/ml-tracking/","title":"Machine Learning Tracking","text":"<p>As an AI-centered platform, tracking of ML activities is essential. Machine Learning Tracking refers to the monitoring and analysis of model performance during training. It involves tracking various metrics like accuracy and loss. This process helps improving the models performance to make it more effective and reliable.</p> <p>We have two solutions available to integrate ML Tracking with your SharingHub workflow:</p> <ul> <li>GitLab Integration</li> <li>SharingHub Integration</li> </ul>"},{"location":"share/ml-tracking/gitlab/","title":"GitLab Integration","text":"<p>Introduced in GitLab <code>15.11</code>, and generally available in GitLab <code>16.1</code>, \"Machine learning model experiments\" allows users to track experiments directly in their GitLab project (under <code>Project (sidebar) &gt; Analyze &gt; Model Experiments</code>), as close as possible to your model code. This feature is relatively new, and is still receiving important feature requests for improvement. On another note, that also means that right now some tooling may be missing when using it.</p> GitLab Experiments <p>The important feature is the MLflow compatibility. That means you can use MLflow to push to GitLab as a tracking URI. You can learn more in the MLflow client compatibility documentation.</p> <p>Note</p> <p>An important thing to note is that your SharingHub may be configured with GitLab integration, and the code generator in the MLflow helper will use GitLab as a tracking URI in the snippets generated. Be sure to check this helper!</p>"},{"location":"share/ml-tracking/mlflow-sharinghub/","title":"MLflow SharingHub","text":"<p>MLflow is a platform to streamline machine learning development, including tracking experiments, packaging code into reproducible runs, and sharing and deploying models.</p> <p>For SharingHub, we developed a plugin for MLflow, MLflow SharingHub, that enables model tracking per AI Model project in SharingHub. To be more clear, each AI model project in SharingHub have a corresponding MLflow tracking URI, like <code>https://sharinghub.example.com/mlflow/&lt;gitlab-project-path&gt;/tracking/</code>.</p>"},{"location":"share/ml-tracking/mlflow-sharinghub/#permissions","title":"Permissions","text":"<p>The permission system is based on the permission system of SharingHub, based upon GitLab. That means only collaborators of the GitLab repository can use the tracking URI, people with read-only access on the project will have read-only access in mlflow, and no access will not display anything.</p> <p>Permissions mapping with GitLab (for experiments, runs, model registry...):</p> <ul> <li>Create: Owner, Maintainer, Developer</li> <li>Read: Owner, Maintainer, Developer, Reporter, Guest</li> <li>Update: Owner, Maintainer, Developer</li> <li>Delete: Owner, Maintainer, Developer</li> </ul>"},{"location":"share/ml-tracking/mlflow-sharinghub/#authentication","title":"Authentication","text":""},{"location":"share/ml-tracking/mlflow-sharinghub/#web","title":"Web","text":"<p>MLflow SharingHub uses SharingHub authentication session, meaning that if you login in SharingHub, you should also be authenticated in MLflow SharingHub.</p>"},{"location":"share/ml-tracking/mlflow-sharinghub/#api","title":"API","text":"<p>You can authenticate with the MLflow API by setting an environment variable, <code>MLFLOW_TRACKING_TOKEN</code>, with an access token as value.</p> <p>The MLflow code generator can help you:</p> <p></p>"},{"location":"share/ml-tracking/mlflow-sharinghub/#limitations","title":"Limitations","text":"<p>It is important to note that there are some limitations and constraints on MLflow SharingHub. It is a plugin, on top of MLflow, meaning MLflow particularities impose restrictions. Despite having multiple tracking URIs, we have a single server and database. In MLflow, experiments names, and model names (model registry) are unique, if I name an experiment, no one can create another experiment with the same name.</p> <p>If you use MLflow SharingHub, and use project A tracking URI, another person on project B could want to use the same name as you for an experiment, creating name collision, throwing an error. MLflow experiments and models (in Model Registry) requires a suffix with the GitLab project ID, of the format <code>&lt;experiment-name&gt; (&lt;project-id&gt;)</code>. This avoid collision across all projects.</p> <p>Note</p> <p>An important thing to note is that your SharingHub may be configured with MLflow SharingHub integration, and the code generator in the MLflow helper will use MLflow SharingHub as a tracking URI in the snippets generated. The experiment name generated in the snippet will also contains automatically the project ID! Be sure to check this helper!</p>"},{"location":"tutorials/dataset_with_dvc/","title":"Dataset with DVC","text":""},{"location":"tutorials/dataset_with_dvc/#introduction","title":"Introduction","text":"<p>The most popular Version Control System, Git, is not adapted for versioning large amounts of data, it is not recommended to commit big files in your repository.</p> <p>When we need to version large amounts of data we use Data Version Control (DVC), that lets you capture the versions of files/directories in your Git repository, while storing them on-premises or in cloud storage. Each DVC \"commit\" updates dvc-specific files, and these modifications can be committed with Git. The real data is then versioned and stored with DVC, while your Git repository references the \"pointer\" to this data. The result is a single history for your source code and data that you can traverse \u2014 a proper journal of your work!</p> <p>The DVC integration offered by SharingHub enables protected access to versioned data, while respecting the management of data access rights carried out on GitLab, making it the central point for information management.</p>"},{"location":"tutorials/dataset_with_dvc/#prerequisites","title":"Prerequisites","text":""},{"location":"tutorials/dataset_with_dvc/#install","title":"Install","text":"<p>First, you must of course install DVC.</p> <p>Follow their documentation: Installation</p>"},{"location":"tutorials/dataset_with_dvc/#git-repository","title":"Git repository","text":"<p>When you have access to the <code>dvc</code> command, you will need to use it in a Git repository. You can create one for this tutorial, or use an existing one.</p> <p>Initialize a repository for the tutorial:</p> <pre><code>git init example-dvc\ncd example-dvc\ntouch README.md\ngit add README.md\ngit commit -m \"Initial commit\"\n# replace with your own GitLab project url\ngit remote add origin https://gitlab.example.com/&lt;project-path&gt;.git\ngit push --set-upstream origin main\n</code></pre>"},{"location":"tutorials/dataset_with_dvc/#gitlab-project-optional","title":"GitLab project (Optional)","text":"<p>If you want to use SharingHub integration with DVC, you will need to push your repository in GitLab. As described here you must add the topic <code>sharinghub:dataset</code> to the project.</p>"},{"location":"tutorials/dataset_with_dvc/#setup-dvc","title":"Setup DVC","text":""},{"location":"tutorials/dataset_with_dvc/#init","title":"Init","text":"<p>The first step is to initialize the DVC configuration.</p> <pre><code>dvc init\n</code></pre> <p>The configuration itself is not ignored by Git, as you need to share it with other users. The authentication credentials on the other hand will be ignored for obvious security purposes.</p>"},{"location":"tutorials/dataset_with_dvc/#configure-remote","title":"Configure remote","text":"<p>You will now need to configure a remote storage and the appropriate authentication.</p>"},{"location":"tutorials/dataset_with_dvc/#sharinghub","title":"SharingHub","text":"<p>You can use SharingHub as the remote storage of DVC. In your project page you can find a code generator to help you for the setup.</p> <p> </p> DVC helper <p>Copy the project's unique identifier (<code>&lt;project_id&gt;</code>) by connecting to the GitLab interface via the URL <code>https://gitlab.example.com/&lt;project-path&gt;</code>.</p> <p></p> <p>This ID is necessary to identify the storage path for DVC, you can continue the configuration.</p> <pre><code># replace with your sharinghub URL and the correct project ID\ndvc remote add --default sharinghub https://sharinghub.example.com/api/store/&lt;project_id&gt;\ndvc remote modify sharinghub auth custom\ndvc remote modify sharinghub custom_auth_header 'X-Gitlab-Token'\n</code></pre> <p>You can commit and push the DVC configuration:</p> <pre><code>git add .\ngit commit -m \"Initialize DVC\"\ngit push origin main\n</code></pre> <p>Finally, configure your authentication credentials with a GitLab access token, you will need at least the <code>read_api</code> permission.</p> <pre><code>dvc remote modify --local sharinghub password &lt;your-personal-gitlab-token&gt;\n</code></pre>"},{"location":"tutorials/dataset_with_dvc/#s3-bucket","title":"S3 bucket","text":"<p>Warning</p> <p>Usage of a custom DVC remote such as your own S3 bucket will impact the easiness of sharing for your project. To be more clear, access to that bucket will require the credentials of the bucket, and it is not tied to our \"GitLab-centered\" philosophy. Be sure to address this problem by properly documenting how to retrieve the credentials.</p> <p>You can alternatively chose to use an S3 bucket for the storage. In order to be able to use this bucket for other repositories use a subpath in the bucket path. It could be the project ID, path, name, slug etc...</p> <pre><code>dvc remote add --default my-bucket s3://&lt;bucket&gt;/&lt;project-identifier&gt;\ndvc remote modify my-bucket endpointurl &lt;s3-endpoint-url&gt;\n</code></pre> <p>Now commit and push the DVC configuration:</p> <pre><code>git add .\ngit commit -m \"Initialize DVC\"\ngit push origin main\n</code></pre> <p>Configure the remote credentials with your S3 access key id and secret access key.</p> <pre><code>dvc remote modify --local my-bucket access_key_id &lt;access-key-id&gt;\ndvc remote modify --local my-bucket secret_access_key &lt;secret-access-key&gt;\n</code></pre>"},{"location":"tutorials/dataset_with_dvc/#usage","title":"Usage","text":""},{"location":"tutorials/dataset_with_dvc/#tracking-data","title":"Tracking data","text":"<p>The use of DVC is simple, but because it it used alongside Git you must always be rigorous and not forget to use it correctly.</p> <p>Let's pick a piece of data to work with. We'll create a file, <code>very_big_file.txt</code>, in the <code>data</code> directory.</p> <pre><code>mkdir data\necho \"Very big content\" &gt; data/very_big_file.txt\n</code></pre> <p>Use <code>dvc add</code> to start tracking the dataset:</p> <pre><code>dvc add data\n</code></pre> <p>DVC stores information about the added file in a special <code>.dvc</code> file named <code>data.dvc</code>. This small, human-readable metadata file acts as a placeholder for the original data for the purpose of Git tracking. You can track files or directories.</p> <p>Next, run the following commands to track changes in Git:</p> <pre><code>git add data.dvc .gitignore\ngit commit -m \"Add data\"\ndvc push\ngit push\n</code></pre> <p>Now, we can modify the data.</p> <pre><code>echo \"Very big content but different\" &gt; data/very_big_file.txt\n</code></pre> <p>Update DVC tracking:</p> <pre><code>dvc add data\n</code></pre> <p>You will notice that <code>data.dvc</code> was modified to reflect that the data changed. To finalize the update, commit and push.</p> <pre><code>git add data.dvc\ngit commit -m \"Modify data\"\ndvc push\ngit push\n</code></pre> <p>By combining Git and DVC, if you go back to the previous commit you can synchronize the data to the previous version.</p> <pre><code>git checkout HEAD~1\ndvc checkout\n</code></pre>"},{"location":"tutorials/dataset_with_dvc/#retrieving-data","title":"Retrieving data","text":"<p>To retrieve the managed data:</p> <ul> <li> <p>Clone the Git project.</p> <pre><code>git clone https://gitlab.example.com/&lt;project-path&gt;\ncd &lt;project-dir&gt;\n</code></pre> </li> <li> <p>Configure authentication credentials as described in the section Configure remote.</p> <pre><code># Credentials setup for SharingHub integration\ndvc remote modify --local sharinghub password &lt;your-personal-gitlab-token&gt;\n</code></pre> </li> <li> <p>Download the data through <code>dvc pull</code>.</p> <pre><code>dvc pull\n</code></pre> </li> </ul>"},{"location":"tutorials/dataset_with_huggingface_interface/","title":"Dataset with HuggingFace interface","text":""},{"location":"tutorials/dataset_with_huggingface_interface/#introduction","title":"Introduction","text":"<p>In this tutorial you will learn how to create a <code>dataset.py</code> for defining a HuggingFace <code>datasets</code> interface.</p> <p>A dataset contains files, most likely divided in directories with a specific layout. The Hugging Face's <code>datasets</code> library allows you to create and load custom datasets efficiently, by offering a standardized Python API interface to interact with the data, making it reusable and shareable.</p> <p>This guide explains how to define a <code>dataset.py</code> file to create a dataset compatible with <code>datasets.load_dataset()</code>.</p>"},{"location":"tutorials/dataset_with_huggingface_interface/#prerequisites","title":"Prerequisites","text":"<p>Before proceeding, make sure you have <code>datasets</code> installed:</p> <pre><code>pip install datasets\n</code></pre>"},{"location":"tutorials/dataset_with_huggingface_interface/#huggingface-interface","title":"HuggingFace interface","text":""},{"location":"tutorials/dataset_with_huggingface_interface/#structure","title":"Structure","text":"<p>Your dataset will include the data itself and a dataset script named <code>dataset.py</code>. This script defines how to download, load, and process data.</p> <p>The Structure of a <code>dataset.py</code> consists of:</p> <ul> <li>Dataset configuration: class that defines configurations, inheriting from <code>datasets.BuilderConfig</code>.</li> <li>Dataset: The main dataset class, inheriting from <code>datasets.GeneratorBasedBuilder</code>.</li> </ul>"},{"location":"tutorials/dataset_with_huggingface_interface/#example-datasetpy","title":"Example <code>dataset.py</code>","text":"<pre><code>import datasets\n\n_DESCRIPTION = \"\"\"A custom image dataset.\"\"\"\n_HOMEPAGE = \"https://example.com\"\n_LICENSE = \"MIT\"\n_CITATION = \"\"\"@article{your_citation, title={Your Dataset} }\"\"\"\n\nclass CustomDatasetConfig(datasets.BuilderConfig):\n    def __init__(self, **kwargs):\n        super(CustomDatasetConfig, self).__init__(**kwargs)\n\nclass CustomDataset(datasets.GeneratorBasedBuilder):\n    BUILDER_CONFIGS = [\n        CustomDatasetConfig(name=\"default\", version=datasets.Version(\"1.0.0\"), description=\"Default config\")\n    ]\n\n    def _info(self):\n        \"\"\"Defines the dataset's schema, including features (columns) and metadata.\"\"\"\n        return datasets.DatasetInfo(\n            description=_DESCRIPTION,\n            features=datasets.Features({\n                \"image\": datasets.Image(),\n                \"label\": datasets.ClassLabel(names=[\"cat\", \"dog\"]),\n            }),\n            supervised_keys=(\"image\", \"label\"),\n            homepage=_HOMEPAGE,\n            citation=_CITATION,\n        )\n\n    def _split_generators(self, dl_manager):\n        \"\"\"Splits the dataset into predefined subsets (for example, train, validation, test).\"\"\"\n        data_dir = \"path/to/your/data\"\n        return [\n            datasets.SplitGenerator(name=datasets.Split.TRAIN, gen_kwargs={\"data_dir\": data_dir}),\n        ]\n\n    def _generate_examples(self, data_dir):\n        \"\"\"Iterates over the dataset files and yields data samples in a structured format.\"\"\"\n        import os\n        for i, filename in enumerate(os.listdir(data_dir)):\n            label = \"cat\" if \"cat\" in filename else \"dog\"\n            yield i, {\"image\": os.path.join(data_dir, filename), \"label\": label}\n</code></pre>"},{"location":"tutorials/dataset_with_huggingface_interface/#usage","title":"Usage","text":"<p>The <code>datasets.load_dataset()</code> function interacts with <code>dataset.py</code> through a series of method calls that define how data is loaded, processed, and structured. Here\u2019s a step-by-step breakdown:</p> <ol> <li> <p>Detecting the Dataset Script:</p> <ul> <li>When calling <code>load_dataset(\"path/to/dataset.py\")</code>, the <code>datasets</code> library identifies and   imports the <code>CustomDataset</code> class defined in <code>dataset.py</code>.</li> </ul> </li> <li> <p>Calling <code>_info()</code>:</p> <ul> <li>The <code>_info()</code> method provides metadata about the dataset, including:</li> <li>Features: Defines the dataset structure (for example, images and labels).</li> <li>Supervised Keys: Specifies input-output pairs for supervised learning.</li> <li>Dataset Citation &amp; Homepage: Provides dataset references.</li> </ul> </li> <li> <p>Calling <code>_split_generators()</code>:</p> <ul> <li>This method is responsible for splitting the dataset into predefined subsets   (for example, <code>train</code>, <code>validation</code>, <code>test</code>).</li> <li>The <code>dl_manager</code> argument can be used to download and extract files if needed.</li> <li>Each split is returned as a <code>datasets.SplitGenerator</code>, which provides parameters   to <code>_generate_examples()</code>.</li> </ul> </li> <li> <p>Calling <code>_generate_examples()</code>:</p> <ul> <li>This method iterates over the dataset files and yields structured examples.</li> <li>The function returns data in a format that matches the schema defined in <code>_info()</code>.</li> <li>Example:</li> </ul> <pre><code>yield i, {\"image\": os.path.join(data_dir, filename), \"label\": label}\n</code></pre> <ul> <li>Each yielded sample becomes an entry in the final dataset.</li> </ul> </li> <li> <p>Using Streaming Mode:</p> <ul> <li>If the dataset is large and does not fit in memory, <code>datasets.load_dataset()</code> can be used to enable streaming mode:</li> </ul> <pre><code>dataset = datasets.load_dataset(\"path/to/dataset.py\", split=\"train\", streaming=True)\n</code></pre> <ul> <li>When streaming is enabled, <code>_generate_examples()</code> is called. Samples are fetched and processed on demand rather than all at once.</li> <li>This is useful for large datasets stored remotely (e.g, in cloud storage)</li> <li>Using trust_remote_code=True with Streaming: If the dataset contains custom processing logic (e.g., special decoding functions), you might need to enable trust_remote_code=True: <code>dataset = load_dataset(\"path/to/dataset.py\", split=\"train\", streaming=True, trust_remote_code=True)</code></li> </ul> </li> </ol>"},{"location":"tutorials/dataset_with_huggingface_interface/#load-dataset-example","title":"Load dataset: example","text":"<pre><code>import datasets\n\ndataset = datasets.load_dataset(\"path/to/dataset.py\", split=\"train\", streaming=True, trust_remote_code=Treu)\nprint(dataset[\"train\"][0])  # Access first training example\n</code></pre> <p>With this flow, <code>dataset.py</code> ensures that <code>datasets.load_dataset()</code> loads and structures data correctly, making it ready for model training and analysis.</p> <p>The <code>datasets.load_dataset()</code> function interacts with <code>dataset.py</code> as follows:</p> <ul> <li>It detects the dataset script and loads the dataset class (<code>CustomDataset</code>).</li> <li>Calls <code>_info()</code> to retrieve dataset metadata.</li> <li>Calls <code>_split_generators()</code> to determine available dataset splits.</li> <li>Calls <code>_generate_examples()</code> to yield examples iteratively.</li> </ul>"},{"location":"tutorials/dataset_with_huggingface_interface/#load-dataset-sen1floods11-dataset","title":"Load dataset: Sen1Floods11-Dataset","text":"<p>For Sen1Floods11-Dataset:</p> <pre><code>from datasets import load_dataset\ntrain_data = load_dataset(\n    \"sen1floods11-dataset/sen1floods11_dataset.py\",\n    split=\"train\",\n    streaming=True,\n    trust_remote_code=True,\n    config_kwargs={\n        \"no_cache\": False,\n        \"context\": \"sen1floods11-dataset/\",\n    },\n)\nprint(next(iter(train_data)))\n</code></pre>"},{"location":"tutorials/dataset_with_huggingface_interface/#dvc-integration","title":"DVC integration","text":"<p>It is possible to use DVC in the dataset.py file to pull your data:</p> <pre><code>from dvc.api import DVCFileSystem\n\nclass CustomBuilderConfig(BuilderConfig):\n    def __init__(self, version=\"1.0.0\", description=None, **kwargs):\n        super().__init__(version=version, description=description)\n        config = kwargs.get(\"config_kwargs\")\n        self.context = config[\"context\"]\n\nclass CustomDataset(datasets.GeneratorBasedBuilder):\n    BUILDER_CONFIG_CLASS = CustomBuilderConfig\n\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n        self.fs = DVCFileSystem(self.config.context)\n\n    ...\n</code></pre> <p>You can instantiate a DVCFileSystem object so you can pull your data with the method <code>self.fs.read_bytes(dvc_path)</code>.</p>"}]}